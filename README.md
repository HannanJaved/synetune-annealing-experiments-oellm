# SyneTune Experiments Workflow

This directory contains scripts and tools for running annealing experiments with adaptive data weighting using SyneTune for hyperparameter optimization. The directory is self-sustained with all necessary scripts and dependencies included.

## Overview

The workflow automates the process of:
1. Sampling optimal data weights based on past evaluation results
2. Running annealing training with the sampled weights
3. Converting checkpoints and launching evaluation jobs

## Files

### `sample_data_weights.py`
- **Purpose**: Samples new data weights for better average accuracy using SyneTune's ConformalQuantileRegression.
- **Input**: `aggregated_eval_results.csv` (aggregated evaluation results from past runs)
- **Output**: Suggested data weights for each data path, normalized to sum to 100.
- **How it works**:
  - Reads CSV and filters to accuracy metrics
  - Trains a model on past configurations and their performance
  - Suggests new weights predicted to improve accuracy

### `prepare_data_weights.sh`
- **Purpose**: Wrapper script to run `sample_data_weights.py` in the correct environment.
- **Actions**:
  - Sources the SyneTune virtual environment (`venv_synetune_experiments`)
  - Runs `sample_data_weights.py`
  - Parses output and saves data paths to `${BASE_PATH}/run6/data_paths.txt`

### `annealing_template.sh`
- **Purpose**: Main SLURM script for running annealing training.
- **Workflow**:
  - Calls `prepare_data_weights.sh` to get adaptive data weights
  - Reads weights from file and sets `DATA_PATHS`
  - Runs Megatron-LM training with the sampled weights
  - Converts checkpoints using Open-Sci tools
  - Launches evaluation jobs with dependency on conversion completion

### `prepare_and_launch_eval.sh`
- **Purpose**: Prepares evaluation by generating model paths and argument files, then launches evaluation job array.
- **Dependencies**: Uses `tasks.txt` and `print_file_paths.py`

### `checkpoint-conversion/` Directory
- Contains scripts for converting Megatron-LM checkpoints to HuggingFace format
- Used by `annealing_template.sh` after training

### `eval-synetune/` Directory
- Contains evaluation scripts (`main_script.sh`, `slurm_script.sh`) for running LM evaluation harness
- Uses `python-args.txt` generated by `prepare_and_launch_eval.sh`

### `tasks.txt`
- Defines evaluation tasks and few-shot settings
- Format: `task_name;num_fewshot`

### `print_file_paths.py`
- Utility script to find and list model checkpoint paths
- Used by `prepare_and_launch_eval.sh`

## Directory Structure

```
synetune-experiments/
├── README.md
├── annealing_template.sh          # Main training script
├── sample_data_weights.py         # Data weight optimization
├── prepare_data_weights.sh        # Wrapper for weight sampling
├── prepare_and_launch_eval.sh     # Evaluation preparation
├── print_file_paths.py            # Utility for finding checkpoints
├── tasks.txt                      # Evaluation task definitions
├── aggregated_eval_results.csv    # Historical evaluation data
├── checkpoint-conversion/         # Checkpoint conversion tools
│   ├── consolidated_conversion_workflow.py
│   ├── mcore_to_hf_opensci.py
│   ├── template.sbatch
│   └── README.md
├── eval-synetune/                 # Evaluation scripts
│   ├── main_script.sh
│   ├── slurm_script.sh
│   ├── model_paths.txt
│   └── python-args.txt
└── initials-runs/                 # Run directories (created during execution)
```

## Self-Sustainability

This directory is completely self-contained with all necessary scripts, dependencies, and utilities. No external files are required - everything needed for the full annealing workflow (data weighting, training, conversion, evaluation) is included.

## Workflow Steps

1. **Prepare Environment**:
   - Ensure `venv_synetune_experiments` is set up with SyneTune and dependencies
   - Ensure `aggregated_eval_results.csv` exists with past run data
   - Set `BASE_PATH` environment variable (e.g., `/leonardo_work/AIFAC_L01_028/hmahadik`)

2. **Run Training**:
   ```bash
   sbatch annealing_template.sh
   ```

3. **Automatic Process**:
   - Script samples new data weights
   - Saves weights to `${BASE_PATH}/run6/data_paths.txt`
   - Runs training with optimized weights
   - Converts checkpoints
   - Submits evaluation job

4. **Monitor**:
   - Check logs in `/leonardo/home/userexternal/hmahadik/logs/synetune-initialruns/run6/`
   - Evaluation results will be in `${LOG_PATH}/eval_results`

## Data Groups

The system optimizes weights for three data groups:
- **nemotron**: High-quality synthetic and actual data (6 paths)
- **finemath**: Math-focused datasets (2 paths)
- **starcoder**: Code datasets (1 path)

Weights are distributed evenly within each group.

## Dependencies

- SyneTune (for optimization)
- Megatron-LM (for training)
- Open-Sci tools (for checkpoint conversion)
- LM Evaluation Harness (for evaluation)

## Customization

- Modify `DATA_GROUPS` in `sample_data_weights.py` to change data sources
- Adjust config space in `sample_data_weights.py` for different weight ranges
- Update paths in `annealing_template.sh` for different environments
- Change hyperparameters in `annealing_template.sh` as needed

## Configuration for New Runs

Before starting a new run, update the following hardcoded values in `annealing_template.sh`:

### Run-Specific Paths and Names
- **Run number**: Change `run6` to the new run number (e.g., `run7`) in:
  - `CHECKPOINT_PATH="${BASE_PATH}/run6/checkpoints"`
  - `TENSORBOARD_DIR="${BASE_PATH}/run6/tensorboard"`
  - `WANDB_DIR="${BASE_PATH}/wandb"`
  - Log paths in SLURM directives
- **WANDB experiment name**: Update `WANDB_EXP_NAME="synetune-initial-runs6"` to match the new run
- **Job name**: Update `#SBATCH --job-name=Nemotron-Synth` if needed

### Training Configuration
- **Training tokens**: `TRAIN_TOKENS=10_000_000_000` (currently 10B tokens)
- **Time limit**: `#SBATCH --time=18:00:00` (currently 18 hours)
- **Learning rate**: `LR=3e-4` and other optimizer settings
- **Batch sizes**: `GLOBAL_BATCH_SIZE=96`, `MICRO_BATCH_SIZE=8`

### File Paths
- **CSV input**: In `sample_data_weights.py`, ensure `aggregated_eval_results.csv` contains the latest evaluation results
- **Data paths file**: In `prepare_data_weights.sh`, the output goes to `${BASE_PATH}/run6/data_paths.txt` - update the run number

### Environment Variables
- **BASE_PATH**: Set this before running (e.g., `export BASE_PATH=/leonardo_work/AIFAC_L01_028/hmahadik`)
- **LOG_PATH**: Update in the conversion section to match the new run

### Tips for Updates
- Use find-and-replace for the run number (e.g., replace all `run6` with `run7`)
- Check that all path references are consistent
- Verify that the CSV file has data from previous runs for optimal weight sampling
